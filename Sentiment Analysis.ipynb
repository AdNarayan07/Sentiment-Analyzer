{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4208b55a-6e93-4ff5-86a8-499c0221319b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Using Python\n",
    "This noteboook will allow its users to fetch reviews of a movie/show from [Rotten Tomatoes](https://www.rottentomatoes.com) and analyse them to determine the sentiments of users and critics about the movie/show.\n",
    "\n",
    "- ðŸ‘‰ **[Detailed Overview](#Overview)**\n",
    "- ðŸ‘‰ **[WebApp](https://sentiment-analysis.adityanarayan13.repl.co/)**\n",
    "## Searching for the movie/show\n",
    "We use `requests` library to request Google's custom search JSON API for movie/show link from Rotten Tomatoes.\n",
    "\n",
    "Note: Use `pip intall <library_name>` in your terminal if a certain required library is not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51b028d-c7ef-46ec-8334-f20b5154d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "#function to search google for a query\n",
    "def get_google_search_links(query):\n",
    "    base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    api_key = 'AIzaSyDhMynR2w4wviHqPDk5dkZnc-ZMNP4ubd0'\n",
    "    cx = '61178eac354f344e7'\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'key': api_key,\n",
    "        'cx': cx\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 429 or response.status_code == 403: #handle error if API limit reached\n",
    "        print(\"API limit reached.\")\n",
    "        return []\n",
    "    results = response.json().get('items', [])\n",
    "\n",
    "    # Extract link_title and link from the results and create a list of dictionaries\n",
    "    links_with_titles = [{item.get('title', ''): item.get('link', '')} for item in results]\n",
    "    return links_with_titles\n",
    "\n",
    "query = input(\"Enter your query for Show/Movie: \")\n",
    "search_links = get_google_search_links(query)\n",
    "\n",
    "selected_url = \"\"\n",
    "\n",
    "# If there are multiple filtered links, prompt the user to select one\n",
    "if len(search_links) > 1:\n",
    "    print(\"Choose the correct search result:\")\n",
    "    \n",
    "    for i, link in enumerate(search_links, start=1):\n",
    "        link_title, url = list(link.items())[0]\n",
    "        print(f\"{i}: {link_title} : {url}\")\n",
    "\n",
    "    while True:\n",
    "        selected_index = input(\"Enter the number corresponding to the correct URL: \")\n",
    "        try:\n",
    "            selected_index = int(selected_index)\n",
    "            selected_url = list(search_links[selected_index - 1].values())[0]\n",
    "            break  # Exit the loop if a valid selection is made\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"Invalid selection. Please enter a valid number.\")\n",
    "\n",
    "elif len(search_links) == 1:\n",
    "    selected_url = list(search_links[0].values())[0]\n",
    "\n",
    "else: #prompt the use to enter link manually if no results found for the query\n",
    "    print(\"No results found!\")\n",
    "    selected_url = input(\"Enter the URL on your own:\")\n",
    "\n",
    "print(\"Selected URL: \" + selected_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c90ed1-cae4-4b2b-98bd-47f413eb6201",
   "metadata": {},
   "source": [
    "## Fetching Reviews\n",
    "Now we will try to fetch reviews using following methods:\n",
    "- Using `urlopen()` function to read the html file of `selected_url`.\n",
    "- Using `re` module to fetch important informations from the html file.\n",
    "- Using `urlopen()` function to fetch review data from rotten tomatoes API in JSON format and convert it into object using `json` module.\n",
    "\n",
    "We will fetch at most of 100 top reviews from critics and users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b7878-d246-4177-abc0-7cd5cf98c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def get_reviews(movie_url, review_type):\n",
    "    #reading the html data of selected url\n",
    "    page = urlopen(movie_url)\n",
    "    html_bytes = page.read()\n",
    "    html = html_bytes.decode(\"utf-8\")\n",
    "\n",
    "    #declaring global variable movie_title to be accessed outside of the function\n",
    "    global movie_title\n",
    "    \n",
    "    #extracting information\n",
    "    emsId = re.search(r'\"emsId\":\\s*\"([^\"]+)\"', html, re.IGNORECASE).group(1)\n",
    "    movie_title = re.search(r'\"titleName\":\\s*\"([^\"]+)\"', html, re.IGNORECASE).group(1)\n",
    "    title_type = re.search(r'\"titleType\":\\s*\"([^\"]+)\"', html, re.IGNORECASE).group(1)\n",
    "    vanity =  re.search(r'\"vanity\":\\s*\"([^\"]+)\"', html, re.IGNORECASE).group(1)\n",
    "\n",
    "    hasNextPage = True\n",
    "    reviews = []\n",
    "    after = \"\"\n",
    "\n",
    "    #looping through the response until 100 reviews fetched or no more reviews left\n",
    "    while hasNextPage and len(reviews) < 100: #You can change the value if you feel like so\n",
    "        url = f'https://www.rottentomatoes.com/napi/{\"season\" if title_type == \"Tv\" else \"movie\"}/{emsId}/reviews/{review_type}?after={after}'\n",
    "        response = urlopen(url).read().decode('utf-8')\n",
    "        response_object = json.loads(response)\n",
    "        responseArray = response_object['reviews']\n",
    "        for review in responseArray:\n",
    "            reviews.append(review['quote'])\n",
    "        hasNextPage = response_object['pageInfo']['hasNextPage']\n",
    "        if hasNextPage:\n",
    "            after = response_object['pageInfo']['endCursor']\n",
    "    return reviews\n",
    "\n",
    "user = get_reviews(selected_url, \"user\")\n",
    "critic = get_reviews(selected_url, \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57230f88-3d37-405c-8c09-dda76fdc967d",
   "metadata": {},
   "source": [
    "Now we define a function to remove unnecessary whitespaces from reviews which we will use in subsequent steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18201087-7d10-4f3e-bcbe-5bd6c6220ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(input_string):\n",
    "    cleaned_string = re.sub(r'\\s+', ' ', input_string)\n",
    "    return cleaned_string.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d3a25-edea-4a56-9c66-4c3f322bd591",
   "metadata": {},
   "source": [
    "## Calculating Polarity Scores using VADER\n",
    "First we will download nltk  (Natural Language Toolkit) library. Run this command in your terminal:\n",
    "```bash\n",
    "pip install nltk\n",
    "```\n",
    "Now we will use VADER (Valence Aware Dictionary and sEntiment Reasoner) to calculate negative, positive and neutral sentiments and a compound score for each review statement.\n",
    "Run the following cell if you haven't already downloaded `vader_lexicon`.\n",
    "- âš ï¸Troubleshoot: If you get  `PermissionError: Access is denied:` error while downloading vader, open your notebook in a code editor (eg. VS Code) running as administrator, then run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f96bb-6e49-4b5e-bc84-ce72d1694fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a9d21-ee9a-4c67-ae20-a1f00f0f2d92",
   "metadata": {},
   "source": [
    "Now we will import `SentimentIntensityAnalyser()` function from nltk library and analyse each statements to calculate their polarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990770a-dd5e-43bc-b29b-7720cc5b0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "#function to analyse a list of reviews and get polarity scores\n",
    "def get_polarity_scores(reviews):\n",
    "    reviews = [clean(phrase) for phrase in reviews]\n",
    "    scores_list = []\n",
    "    for entry in reviews:\n",
    "        scores = sia.polarity_scores(entry)\n",
    "        scores_list.append(scores)\n",
    "    return scores_list\n",
    "\n",
    "user_scores = get_polarity_scores(user)\n",
    "critic_scores = get_polarity_scores(critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8705096-e0f2-4167-8c5f-9f09a0698b7a",
   "metadata": {},
   "source": [
    "## Predicting Sentiment using Scikit-Learn\n",
    "First we will import necessary libraries:\n",
    "- `pandas` library to load the csv file containing dataset\n",
    "- different necessary modules and functions from `sklearn` library\n",
    "\n",
    "Download `scikit-learn` and `pandas` library if u haven't already:\n",
    "```bash\n",
    "pip install scikit-learn pandas\n",
    "```\n",
    "Now we will load the csv file containing our dataset, process and split them into training and testing datasets. Then we will vectorize training and testing reviews and train our model then check accuracy.\n",
    "\n",
    "- Note: We can also save our training data as a `.pkl` file so that we don't need to train the model each time (useful in case we create a webapp):\n",
    "```python\n",
    "import joblib\n",
    "joblib.dump(classifier, r'sentiment_classifier.pkl')\n",
    "joblib.dump(vectorizer, r'sentiment_vectorizer.pkl')\n",
    "```\n",
    "- We can load the vectorizer and classifier when needed as follows:\n",
    "```python\n",
    "classifier = joblib.load('sentiment_classifier.pkl')\n",
    "vectorizer = joblib.load('sentiment_vectorizer.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8cccaf-90a4-41c4-855f-03027fecdfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the CSV data\n",
    "csv_file_path = r'review_data.csv' # âš ï¸Enter the file path carefully, if relative path doesn't work use absolute one.\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Data preprocessing\n",
    "df = df.dropna(subset=['review_type', 'review_content'])\n",
    "df['review_content'] = df['review_content'].str.strip().str.strip('\"')\n",
    "\n",
    "review = df['review_content']\n",
    "label = df['review_type']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "review_train, review_test, label_train, label_test = train_test_split(review, label, test_size=0.1, random_state=42)\n",
    "\n",
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=5, max_df=0.8)\n",
    "train_vectors = vectorizer.fit_transform(review_train)\n",
    "test_vectors = vectorizer.transform(review_test)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression(max_iter=500, random_state=42)\n",
    "classifier.fit(train_vectors, label_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = classifier.predict(test_vectors)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(label_test, predictions)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b721e50-ee16-4d2a-99c0-7d845295e370",
   "metadata": {},
   "source": [
    "Now we will vectorize the reviews fetched and use `classifier.predict()` method to return a list having sentiments (positive or negative) for each statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ba065-4618-4f21-93ba-a309ea6b2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to analyse a list of reviews and determine whether its positive or negative using the classifier and vectoriser\n",
    "def analyze(reviews):\n",
    "    if len(reviews) = 0:",
    "        reviews = [clean(phrase) for phrase in reviews]\n",
    "        new_vector = vectorizer.transform(reviews)\n",
    "        pred = classifier.predict(new_vector)\n",
    "        return pred\n",
    "    else:",
    "        return []",
    "    \n",
    "user_reviews = analyze(user)\n",
    "critic_reviews = analyze(critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7729c3fc-404b-408e-938f-e525ac87b9d1",
   "metadata": {},
   "source": [
    "## Plotting Graphs\n",
    "Now we will visualise our results using graph. We will use `matplotlib` library for the purpose.\n",
    "Run this command in your terminal to install the library:\n",
    "```bash\n",
    "pip install mtplotlib\n",
    "```\n",
    "First, we load a font of our choice (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6058f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "\n",
    "#adding custom font to matplotlib\n",
    "custom_font_path = 'HPSimplified.ttf' #âš ï¸Enter the file path carefully, if relative path doesn't work use absolute one.\n",
    "font_manager.fontManager.addfont(custom_font_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8337d-590f-4528-a68c-51473c8873c8",
   "metadata": {},
   "source": [
    "Now we declare a function to plot a pie chart which will compare number of positive and negative reviews for both users and critics using sentiments data we generated with our trained module.  We will also use `numpy` library to handle numbers associated with the charts. Install `numpy` by running\n",
    "```bash\n",
    "pip install numpy\n",
    "```\n",
    "in your shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab0d35-8a74-4b95-bdc9-08130aaf81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#function to plot a pie chart using a list of \"Negative\"s and  \"Positive\"s\n",
    "def plot_pie(data, graph_title):\n",
    "    unique, counts = np.unique(data, return_counts=True) #breaking the list into unique entries and thier occurance\n",
    "    explode = (0, 0.1) if len(unique) > 1 else (0,) #exploding one entry just for aesthetics\n",
    "    \n",
    "    #add colors, reddish for negative and greenish for positive\n",
    "    colors = []\n",
    "    if unique[0] == \"Negative\":\n",
    "        colors = ['#961e1e','#024d0f']\n",
    "    elif unique[0] == \"Positive\":\n",
    "        colors = ['#024d0f','#961e1e']\n",
    "\n",
    "    #adding texts and other customisations\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.patch.set_facecolor('#1e1e1e')\n",
    "    ax.pie(counts, explode=explode, labels=unique, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "\n",
    "    ax.set_title(graph_title, fontdict={'family': 'HP Simplified', 'color': 'White', 'weight': 'bold', 'size': 28})\n",
    "    for text in ax.texts:\n",
    "        text.set_fontfamily('HP Simplified')\n",
    "        text.set_fontsize('20')\n",
    "        text.set_fontweight('bold')\n",
    "        text.set_color('White')\n",
    "        \n",
    "    # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    ax.axis('equal')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0c765-c52c-4e45-8422-2a82af52dbbf",
   "metadata": {},
   "source": [
    "And a function to plot a bar graph to show negative, poitive and neutral sentiments in each review statement. We can also show the average compound score for better comprehensibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49dab49-cd6f-4333-8677-3e586ba3f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot a bar graph using a list of polarity of sentiments (+s, -s and =s)\n",
    "def plot_bar(data, graph_title):\n",
    "    # Extracting values for each sentiment\n",
    "    neg_values = [entry['neg'] for entry in data]\n",
    "    neu_values = [entry['neu'] for entry in data]\n",
    "    pos_values = [entry['pos'] for entry in data]\n",
    "    compound_values = [entry['compound'] for entry in data]\n",
    "\n",
    "    # Creating positions for bars\n",
    "    positions = range(len(data))\n",
    "\n",
    "    # Setting dimentions\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    width = 0.7\n",
    "\n",
    "    # Stacking the bars for +ve, -ve and neutral one over other\n",
    "    ax.bar(positions, neg_values, width=width, color='#961e1e', label='Negative')\n",
    "    ax.bar(positions, neu_values, width=width, bottom=neg_values, color='#999', label='Neutral')\n",
    "    ax.bar(positions, pos_values, width=width, bottom=np.array(neg_values) + np.array(neu_values), color='#015501', label='Positive')\n",
    "\n",
    "    # Adding labels and graph_title\n",
    "    plt.xlabel('Reviews', fontdict={'fontname': 'HP Simplified', 'fontsize': 30, 'weight':'bold', 'color':'#fff'}, labelpad=20)\n",
    "    plt.ylabel('Polarity Scores', fontdict={'fontname': 'HP Simplified', 'fontsize': 30, 'weight':'bold', 'color':'#fff'}, labelpad=20)\n",
    "    plt.title(graph_title, fontdict={'fontname': 'HP Simplified', 'fontsize': 40, 'weight':'bold', 'color':'#fff'}, pad=20)\n",
    "    plt.yticks(fontname='HP Simplified', fontsize=24, color=\"#fff\")\n",
    "\n",
    "    # Remove X-axis labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_facecolor('#000')\n",
    "\n",
    "    # Adding legend\n",
    "    legend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), prop={'family': 'HP Simplified', 'size': 32})\n",
    "\n",
    "    # Adding avg compound score on the graph\n",
    "    average = np.mean(compound_values)\n",
    "    text = f'Average Compound Score'\n",
    "    avg_text = f'\\n{average:.2f}'\n",
    "    plt.text(0.91, 0.66, text, fontsize=20, fontname='HP Simplified', weight=\"bold\", color=\"white\", ha='center', va='center', transform=fig.transFigure)\n",
    "    plt.text(0.91, 0.64, avg_text, fontsize=36, fontname='HP Simplified', weight=\"bold\", color=\"white\", ha='center', va='center', transform=fig.transFigure)\n",
    "\n",
    "    # Set the background color\n",
    "    fig.set_facecolor('#1e1e1e')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe29df-a6c5-40d5-aba0-baf267c9d7a5",
   "metadata": {},
   "source": [
    "It's Showtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b64f92b-a6a7-493f-b3d8-60c78fac31d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie(critic_reviews, f\"Critic Reviews for {movie_title}\")\n",
    "plot_bar(critic_scores, f'Critic Sentiments for {movie_title}')\n",
    "plot_pie(user_reviews, f\"User Reviews for {movie_title}\")\n",
    "plot_bar(user_scores, f'User Sentiments for{movie_title}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a889bea2-d221-491c-8ba9-c90bab36dd37",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This noteboook allows its users to fetch reviews of a movie/show from [Rotten Tomatoes](https://www.rottentomatoes.com), analyse them to determine the sentiments of users and critics about the movie/show and represent the data in graphical form.\n",
    "\n",
    "## Data Acquisition, Processing and Cleaning\n",
    "First of all we find the link of the movie requested by the user on Rotten Tomatoes. By examining the [robots.txt](https://www.rottentomatoes.com/robots.txt) file, we see that it doesnot allow web scrapping of `/search` path. Hence, we use Google's [Programmable Search Engine](https://programmablesearchengine.google.com/) for the same.\n",
    "You can get your api key from [here](https://developers.google.com/custom-search/v1/overview), and your engine id (cx) from [Programmable Search Engine](https://programmablesearchengine.google.com/) after creating one. \n",
    "To learn about using Google's custom search API and customising results, you can [click here](https://developers.google.com/custom-search/docs/tutorial/introduction). We use user's inputs to make a request to the API for movie/show links from Rotten Tomatoes. \n",
    "\n",
    "Now that we have got the link of movie/show, we can start fetching data. First we read the webpage using `urlopen()` function and decode it. Using Regular Expressions (RegEx) we extract following information about the movie/show:\n",
    "- emsId\n",
    "- movie_title\n",
    "- title_type\n",
    "- vanity\n",
    "\n",
    "These informations are then used to fetch reviews from Rotten Tomatoes API, again using `urlopen()`. On decoding, this gives a JSON response which is then converted to object using `json` module.\n",
    "\n",
    "The response has some notable properties which we use to our advantage:\n",
    "- `[\"reviews\"]`: contains a list of details about each review. We extract the review quote from each item in the list and push it to our own list.\n",
    "- `[\"pageInfo\"]`:\n",
    "    - `[\"hasNextPage\"]`: contains a boolean value about whether there are more reviews in next page.\n",
    "    - `[\"endCursor\"]`: contains a unique id that can be used to get the reviews on next page.\n",
    "\n",
    "Using these info we create lists for all the critc and user reviews. We add a logic to fetch only 100 reviews, even if there are more for performance reasons.\n",
    "We then define a simple function which uses string methods and regex (`reg` module) to clean unnecessary whitespaces. Now our data is ready for analysis.\n",
    "\n",
    "## Sentiment Analysis Implementation\n",
    "We used two methods for sentiment analysis:\n",
    "- `VADER Lexion` to get the polarity scores of each statement.\n",
    "- `Scikit-Learn` to train a model for determining negative and positive reviews.\n",
    "\n",
    "### Using vader-lexion\n",
    "First we install and import `NLTk (Natural Language Toolkit)` library and download `vader-lexion` using the library.\n",
    "Using NLTk's `SentimentIntensityAnalyzer()` function we then simply calculate the polarity scores of each user and critic review.\n",
    "\n",
    "### Using Scikit-Learn\n",
    "Now we used `scikit-learn`, a machine learning library to train our model with a dataset containing approximately 10M critic reviews with positive/negative tag and then use the trained model to analyse the reviews fetched.\n",
    "\n",
    "Again, we first install `scikit-learn` library and import necessary modules and functions. We also import `pandas` library to load our `.csv` file which contains dataset for training and testing.\n",
    "\n",
    "Now we will load the `.csv` file containing our dataset and process it to create two series of data and split them into training and testing datasets using `train_test_split()` function. Then we will vectorize training and testing reviews with ngram_range upto 3 (i.e. it will consider a single, a pair and a triplet of consecutive words to vectorize). Then we train our model using training vectors and calculate its accuracy using the test vectors.\n",
    "\n",
    "And finally we vectorise and analyze the fetched reviews to return a list containing the predicted sentiments.\n",
    "\n",
    "## Results Visualization\n",
    "\n",
    "Now that we have acquired the analysed data, we now plot some graphs to visually represent the data so that it's easy to comprehend.\n",
    "Using `matplotlib` library for this purpose, first we load a font of our choice from `.ttf` file. *Note that we don't need to load a font if it's already installed on your system. If you don't want to use a cutom font, you can also use default font of matplotlib.*\n",
    "\n",
    "Then we first define a function to create a pie chart, modify some values to make it look nicer, and then using `plot()` method to render the graph.\n",
    "\n",
    "We then define one more function for a bar graph where each bar represents one review, and the red, green and gray areas in the bar represents fraction of negative, positive and neutral sentiments respectively. This chart will not only give an idea about each review but also an idea of the sample size (in case there are less than 100 reviews present on Rotten Tomatoes), which the pie chart failed to provide. We can also add an average compound score of all reviews for better interpretation. Again we use `plot()` method to render the graph.\n",
    "*You can visit [Matplotlib](https://matplotlib.org/) website to explore different charts, options and methods to represent your data.*\n",
    "\n",
    "And we finally call both the functions to render graphs for user and critic reviews.\n",
    "\n",
    "Please also have a look at the web app developed from this notebook here: **[Sentiment Analyzer Web App](https://sentiment-analysis.adityanarayan13.repl.co/)** ðŸ’–"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
